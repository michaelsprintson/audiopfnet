{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py as  h\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import skimage.transform as skt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttrDict(dict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(AttrDict, self).__init__(*args, **kwargs)\n",
    "        self.__dict__ = self\n",
    "args = AttrDict({\"num_trajs\":807,\n",
    "        \"sl\":96,\n",
    "        \"batch_size\":20,\n",
    "        \"chan_size\":3,\n",
    "        \"num_particles\":30,\n",
    "        \"window_length\":6,\n",
    "        \"h\":64,\n",
    "        \"img_size\":(132,96),\n",
    "        \"emb_obs\":32,\n",
    "        \"emb_act\":32,\n",
    "        \"map_emb\":32,\n",
    "        \"dropout\":0.1, #we changed this from 0.5\n",
    "        \"obs_num\":5,\n",
    "        \"resamp_alpha\":0.5,\n",
    "        \"bp_length\":10, \n",
    "        \"clip\":1.0,\n",
    "        \"lr\":0.0005,\n",
    "        \"bpdecay\":0.1,\n",
    "        \"l2_weight\":1.0,\n",
    "        \"l1_weight\":0.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes = []\n",
    "def test(d, f, i):\n",
    "    hh = h.File(dataloc + folder + \"/\" + i,'r')['feature']\n",
    "    exfile = skt.resize(hh, (3, 132, hh.shape[2]))\n",
    "    shapes.append(exfile.shape)\n",
    "    return exfile\n",
    "dataloc = \"data/DCASE_2019_feature/\"\n",
    "folders = [i for i in os.listdir(dataloc) if not \".\" in i]\n",
    "data = {}\n",
    "for folder in folders:\n",
    "    dataset = {i:test(dataloc, folder, i) for i in os.listdir(dataloc + folder)}\n",
    "    data[folder] = dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = list(data['gcc_mic_dev'].keys())\n",
    "e = {key:{k:data[k][key] for k in data if key in data[k]} for key in keys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_order = []\n",
    "labels = []\n",
    "with open(\"data/DCASE_2019_feature/DCSSE_2019_labels.txt\", 'r') as lbl:\n",
    "    for line in lbl.readlines():\n",
    "        line = line.split()\n",
    "        labels.append((int(line[2]),int(line[3])))\n",
    "        labels_order.append(line[0])\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_order = ['gcc_mic_dev', 'logmel_mic_dev', 'intensity_foa_dev', 'logmel_foa_dev']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.zeros((labels.shape[0], len(feat_order), args.chan_size, *args.img_size))\n",
    "for lblidx, lbl in enumerate(labels_order):\n",
    "    inputs[lblidx] = np.stack([e[lbl + \".h5\"][k] for k in feat_order])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_model import AudioDataset, AudioLocalizer\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = AudioDataset(inputs, labels)\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(ds, [(train_size := len(ds)*4//5), len(ds) - train_size],generator=torch.Generator().manual_seed(42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=20,\n",
    "                            num_workers=8, pin_memory=True, shuffle=False, drop_last = True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=20,\n",
    "                            num_workers=8, pin_memory=True, shuffle=False, drop_last = True)\n",
    "model = AudioLocalizer(args)\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=args.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = enumerate(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> torch.Size([20, 3, 132, 96])\n",
      "embedding.shape torch.Size([20, 1, 256])\n",
      "embedding.shape torch.Size([600, 1, 256])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 11\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mtype\u001b[39m(foa), foa\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     10\u001b[0m model\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 11\u001b[0m loss, log_loss, particle_pred \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mstep(\n\u001b[1;32m     12\u001b[0m     gcc, mic, intensity, foa, lbl, args)\n\u001b[1;32m     13\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     14\u001b[0m \u001b[39mif\u001b[39;00m args\u001b[39m.\u001b[39mclip \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/Documents/nosync.nosync/audio-pfnet/custom_model.py:130\u001b[0m, in \u001b[0;36mAudioLocalizer.step\u001b[0;34m(self, gcc_mic, logmel_mic, intensity_foa, logmel_foa, gt_pos, args)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, gcc_mic, logmel_mic, intensity_foa, logmel_foa, gt_pos, args):\n\u001b[1;32m    128\u001b[0m     pred, particle_pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(gcc_mic, logmel_mic, intensity_foa, logmel_foa)\n\u001b[0;32m--> 130\u001b[0m     gt_xy_normalized \u001b[39m=\u001b[39m gt_pos[:, :, :\u001b[39m2\u001b[39;49m] \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmap_size\n\u001b[1;32m    131\u001b[0m     gt_theta_normalized \u001b[39m=\u001b[39m gt_pos[:, :, \u001b[39m2\u001b[39m:] \u001b[39m/\u001b[39m (np\u001b[39m.\u001b[39mpi \u001b[39m*\u001b[39m \u001b[39m2\u001b[39m)\n\u001b[1;32m    132\u001b[0m     gt_normalized \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([gt_xy_normalized, gt_theta_normalized], dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "for iteration, data in f:\n",
    "\n",
    "    gcc, mic, intensity, foa, lbl = data\n",
    "\n",
    "    print(type(foa), foa.shape)\n",
    "\n",
    "\n",
    "    model.zero_grad()\n",
    "    loss, log_loss, particle_pred = model.step(\n",
    "        gcc, mic, intensity, foa, lbl, args)\n",
    "    loss.backward()\n",
    "    if args.clip > 0:\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 ('audiovenv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a8d22213adda706d8f027868d89acd6d34ca8de79ce52df589e4ccba5542eb6b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
